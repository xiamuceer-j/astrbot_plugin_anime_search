import os
import json
from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register
from astrbot.api import logger
from astrbot.api.message_components import Plain, Image, Share
import aiohttp
from bs4 import BeautifulSoup


@register("anime_search", "xiamuceer-j", "AGEåŠ¨æ¼«ç•ªå‰§æœç´¢æ’ä»¶", "1.0.0")
class AnimeSearchPlugin(Star):
    def __init__(self, context: Context):
        super().__init__(context)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36',
            'Cookie': 'Hm_lvt_7fdef555dc32f7d31fadd14999021b7b=1743995269; HMACCOUNT=7684B2F2E92F5605; notice=202547; cleanMode=0; Hm_lpvt_7fdef555dc32f7d31fadd14999021b7b=1743995601'
        }
        self.cache_dir = os.path.join(os.getcwd(), 'search_cache')
        os.makedirs(self.cache_dir, exist_ok=True)

    def _get_cache_path(self, user_id: str) -> str:
        return os.path.join(self.cache_dir, f"{user_id}.json")

    def _save_cache(self, user_id: str, data: dict):
        cache_path = self._get_cache_path(user_id)
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _load_cache(self, user_id: str) -> dict:
        cache_path = self._get_cache_path(user_id)
        if not os.path.exists(cache_path):
            return None
        with open(cache_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    @filter.command("æŸ¥ç•ª")
    async def search_anime(self, event: AstrMessageEvent):
        '''æŸ¥è¯¢AGEåŠ¨æ¼«ç•ªå‰§ä¿¡æ¯\nç”¨æ³•ï¼š/æŸ¥ç•ª ç•ªå‰§åç§°'''
        args = event.message_str.split(maxsplit=1)
        if len(args) < 2:
            yield event.plain_result("è¯·è¾“å…¥è¦æŸ¥è¯¢çš„ç•ªå‰§åç§°ï¼Œä¾‹å¦‚ï¼š/æŸ¥ç•ª é®å¤©")
            return

        keyword = args[1]
        try:
            html = await self._fetch_search_results(keyword)
            result = self._parse_results(html, keyword)
            anime_list = result['ç•ªå‰§åˆ—è¡¨']
            total = len(anime_list)

            if total == 0:
                yield event.plain_result(f"æœªæ‰¾åˆ°ä¸ã€Œ{keyword}ã€ç›¸å…³çš„ç•ªå‰§")
                return

            if total > 2:
                page_size = 2
                total_pages = (total + page_size - 1) // page_size
                cache_data = {
                    "keyword": keyword,
                    "all_results": anime_list,
                    "total_pages": total_pages,
                    "current_page": 1,
                    "page_size": page_size
                }
                self._save_cache(event.get_sender_id(), cache_data)

                yield event.plain_result(f"ğŸ”æ‰¾åˆ°{total}æ¡ç»“æœï¼ˆç¬¬1/{total_pages}é¡µï¼‰")
                for anime in anime_list[:page_size]:
                    yield event.chain_result(self._build_anime_message(anime))
                yield event.plain_result("è¾“å…¥ /ä¸‹ä¸€é¡µ ç»§ç»­æŸ¥çœ‹ï¼Œ/ä¸Šä¸€é¡µ è¿”å›")
            else:
                yield event.plain_result(f"æ‰¾åˆ°{total}æ¡ç»“æœï¼š")
                for anime in anime_list:
                    yield event.chain_result(self._build_anime_message(anime))

        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {str(e)}", exc_info=True)
            yield event.plain_result("ç•ªå‰§æŸ¥è¯¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•")

    @filter.command("ä¸‹ä¸€é¡µ")
    async def next_page(self, event: AstrMessageEvent):
        '''æŸ¥çœ‹ä¸‹ä¸€é¡µæœç´¢ç»“æœ'''
        cache = self._load_cache(event.get_sender_id())
        if not cache:
            yield event.plain_result("è¯·å…ˆä½¿ç”¨ã€æŸ¥ç•ªã€‘è¿›è¡Œæœç´¢")
            return

        current_page = cache['current_page'] + 1
        if current_page > cache['total_pages']:
            yield event.plain_result("å·²ç»æ˜¯æœ€åä¸€é¡µäº†")
            return

        start = (current_page - 1) * cache['page_size']
        page_data = cache['all_results'][start:start + cache['page_size']]

        cache['current_page'] = current_page
        self._save_cache(event.get_sender_id(), cache)

        yield event.plain_result(f"ğŸ“–ç¬¬{current_page}/{cache['total_pages']}é¡µ")
        for anime in page_data:
            yield event.chain_result(self._build_anime_message(anime))
        if current_page < cache['total_pages']:
            yield event.plain_result("è¾“å…¥ã€ä¸‹ä¸€é¡µã€‘ç»§ç»­æŸ¥çœ‹ï¼Œã€ä¸Šä¸€é¡µã€‘è¿”å›")

    @filter.command("ä¸Šä¸€é¡µ")
    async def prev_page(self, event: AstrMessageEvent):
        '''æŸ¥çœ‹ä¸Šä¸€é¡µæœç´¢ç»“æœ'''
        cache = self._load_cache(event.get_sender_id())
        if not cache:
            yield event.plain_result("è¯·å…ˆä½¿ç”¨ã€æŸ¥ç•ªã€‘è¿›è¡Œæœç´¢")
            return

        current_page = cache['current_page'] - 1
        if current_page < 1:
            yield event.plain_result("å·²ç»æ˜¯ç¬¬ä¸€é¡µäº†")
            return

        start = (current_page - 1) * cache['page_size']
        page_data = cache['all_results'][start:start + cache['page_size']]

        cache['current_page'] = current_page
        self._save_cache(event.get_sender_id(), cache)

        yield event.plain_result(f"ğŸ“–ç¬¬{current_page}/{cache['total_pages']}é¡µ")
        for anime in page_data:
            yield event.chain_result(self._build_anime_message(anime))
        yield event.plain_result("è¾“å…¥ã€ä¸‹ä¸€é¡µã€‘ç»§ç»­æŸ¥çœ‹ï¼Œã€ä¸Šä¸€é¡µã€‘è¿”å›")

    def _build_anime_message(self, anime: dict) -> list:
        components = []
        if anime.get('å°é¢å›¾'):
            components.append(Image(url=anime['å°é¢å›¾'], file=anime['å°é¢å›¾']))

        text_components = [
            Plain(text=f"ğŸ“º æ ‡é¢˜ï¼š{anime['æ ‡é¢˜']}"),
            Plain(text=f"â± é¦–æ’­ï¼š{anime['é¦–æ’­æ—¶é—´']}"),
            Plain(text=f"ğŸ“ ç®€ä»‹ï¼š{anime['ç®€ä»‹'][:100]}..."),
            Plain(text=f"ğŸ”— è¯¦æƒ…ï¼š{anime['è¯¦æƒ…é“¾æ¥']}")
        ]

        if anime.get('æ’­æ”¾é“¾æ¥'):
            text_components.append(Plain(text=f"â–¶ï¸ æ’­æ”¾ï¼š{anime['æ’­æ”¾é“¾æ¥']}"))

        return components + text_components

    async def _fetch_search_results(self, keyword: str) -> str:
        url = 'https://www.agedm.org/search'
        async with aiohttp.ClientSession(headers=self.headers) as session:
            async with session.get(url, params={'query': keyword, 'page': 1}) as resp:
                resp.raise_for_status()
                return await resp.text()

    def _parse_results(self, html: str, keyword: str) -> dict:
        soup = BeautifulSoup(html, 'html.parser')
        anime_list = []

        for item in soup.find_all('div', class_='cata_video_item'):
            title_tag = item.find('h5')
            if not title_tag:
                continue

            cover_img = item.find('img', class_='video_thumbs')
            play_btn = item.find('a', class_='btn-danger')

            anime = {
                'æ ‡é¢˜': title_tag.text.strip(),
                'è¯¦æƒ…é“¾æ¥': title_tag.a['href'] if title_tag.a else '',
                'é¦–æ’­æ—¶é—´': self._extract_detail(item, 'é¦–æ’­æ—¶é—´'),
                'ç®€ä»‹': self._extract_detail(item, 'ç®€ä»‹'),
                'å°é¢å›¾': cover_img['data-original'] if cover_img else '',
                'æ’­æ”¾é“¾æ¥': play_btn['href'] if play_btn else ''
            }
            anime_list.append(anime)

        return {'ç•ªå‰§åˆ—è¡¨': anime_list}

    def _extract_detail(self, soup, field: str) -> str:
        field_span = soup.find('span', string=lambda t: t and t.strip().startswith(f"{field}ï¼š"))
        if not field_span:
            return ''

        detail_div = field_span.find_parent('div', class_='video_detail_info')
        field_span.extract()
        return detail_div.get_text(strip=True)

    async def terminate(self):
        '''æ¸…ç†èµ„æº'''
        pass